{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0WAC6jk/MXfyhNK5m3WnL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryanGupta0419/PytorchTutorial/blob/main/DLLAB4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p0Od2PfShye"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3SlN91tTLE-",
        "outputId": "ffefd3f8-7076-4ff8-a131-019dba0f22c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.10)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "dataset_url = 'https://www.kaggle.com/datasets/yasserh/titanic-dataset'\n",
        "od.download(dataset_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2bgroMVTTnG",
        "outputId": "5274a267-a186-4b96-dad3-77665da3bba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: aryangupta0419\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/yasserh/titanic-dataset\n",
            "Downloading titanic-dataset.zip to ./titanic-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22.0k/22.0k [00:00<00:00, 18.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/titanic-dataset/Titanic-Dataset.csv')\n",
        "\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
        "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "\n",
        "feature = df.columns[2:]\n",
        "target = df.columns[1]\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "df = df.drop(['Name','Ticket','Cabin'], axis = 1)\n",
        "\n",
        "x= df.drop('Survived', axis = 1)\n",
        "y=df['Survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "LG_71BQdTbwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = X_train.shape[1]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(TitanicModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(64, 48)\n",
        "        self.fc3 = nn.Linear(48,32)\n",
        "        self.fc4 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "class TitanicModelNoDropout(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(TitanicModelNoDropout, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 48)\n",
        "        self.fc3 = nn.Linear(48, 32)\n",
        "        self.fc4 = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ap87Nhq1U0lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TitanicModel(input_size=X_train.shape[1]).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01) #weight decay is how we add L2 regularization\n",
        "\n",
        "\n",
        "model1 = TitanicModelNoDropout(input_size=X_train.shape[1]).to(device)\n",
        "\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001, weight_decay=0.01)"
      ],
      "metadata": {
        "id": "troUD1shU2vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 5000\n",
        "\n",
        "for epochs in range(epoch):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  output = model(X_train_tensor)\n",
        "  loss = criterion(output, y_train_tensor)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if((epochs+1)%500 ==0):\n",
        "    print(f\"Epoch: {epochs+1}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2yFuY-LWdmq",
        "outputId": "a00bc5d4-6491-422e-b544-f77c232bcec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 500, Loss: 0.31626737117767334\n",
            "Epoch: 1000, Loss: 0.23495043814182281\n",
            "Epoch: 1500, Loss: 0.199764221906662\n",
            "Epoch: 2000, Loss: 0.1918654441833496\n",
            "Epoch: 2500, Loss: 0.22140440344810486\n",
            "Epoch: 3000, Loss: 0.1864721179008484\n",
            "Epoch: 3500, Loss: 0.2231733798980713\n",
            "Epoch: 4000, Loss: 0.17551876604557037\n",
            "Epoch: 4500, Loss: 0.23070918023586273\n",
            "Epoch: 5000, Loss: 0.19869032502174377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 5000\n",
        "\n",
        "for epochs in range(epoch):\n",
        "  model1.train()\n",
        "  optimizer1.zero_grad()\n",
        "\n",
        "  output = model1(X_train_tensor)\n",
        "  loss = criterion(output, y_train_tensor)\n",
        "  loss.backward()\n",
        "  optimizer1.step()\n",
        "\n",
        "  if((epochs+1)%500 ==0):\n",
        "    print(f\"Epoch: {epochs+1}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tizBlioakNQ",
        "outputId": "0e079464-0f45-440d-b704-73a9ce81f278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 500, Loss: 0.07667399942874908\n",
            "Epoch: 1000, Loss: 0.0616593174636364\n",
            "Epoch: 1500, Loss: 0.05795777216553688\n",
            "Epoch: 2000, Loss: 0.05700553581118584\n",
            "Epoch: 2500, Loss: 0.05651780217885971\n",
            "Epoch: 3000, Loss: 0.0564345121383667\n",
            "Epoch: 3500, Loss: 0.05640155449509621\n",
            "Epoch: 4000, Loss: 0.056336864829063416\n",
            "Epoch: 4500, Loss: 0.05629561096429825\n",
            "Epoch: 5000, Loss: 0.056249428540468216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(model, X_test_tensor, y_test_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(X_test_tensor)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        correct = (predicted == y_test_tensor).sum().item()\n",
        "        accuracy = correct / len(y_test_tensor)\n",
        "    return accuracy\n",
        "\n",
        "test_accuracy_model = evaluate_accuracy(model, X_test_tensor, y_test_tensor)\n",
        "print(f\"Test Accuracy with Dropout: {test_accuracy_model * 100:.2f}%\")\n",
        "\n",
        "test_accuracy_model1 = evaluate_accuracy(model1, X_test_tensor, y_test_tensor)\n",
        "print(f\"Test Accuracy without Dropout: {test_accuracy_model1 * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R29YYZLbkYU",
        "outputId": "a5bab858-fddc-4815-e557-616717e5be36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy with Dropout: 70.27%\n",
            "Test Accuracy without Dropout: 64.86%\n"
          ]
        }
      ]
    }
  ]
}